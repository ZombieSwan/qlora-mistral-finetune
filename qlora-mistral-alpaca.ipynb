{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1YFz97O7zvjFpal0fFQ3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZombieSwan/qlora-mistral-finetune/blob/main/qlora-mistral-alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zELK9JgMEvwc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft datasets bitsandbytes accelerate --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "F7HZmspKFG3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n"
      ],
      "metadata": {
        "id": "svMMoOJfJlfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "with open(\"alpaca_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_list(data)\n"
      ],
      "metadata": {
        "id": "26K8vI7wKdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(example):\n",
        "    if example[\"input\"]:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
        "    return {\"text\": prompt + example[\"output\"]}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n"
      ],
      "metadata": {
        "id": "QrPU9LxFKhTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ],
      "metadata": {
        "id": "68XBucn1KlG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We have:\n",
        "\n",
        "âœ… Downloaded and loaded Alpaca dataset\n",
        "\n",
        "âœ… Formatted it as instruction â†’ response\n",
        "\n",
        "âœ… Tokenized it for your model"
      ],
      "metadata": {
        "id": "8gDORZYFLICk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up the Data Collator\n",
        "The model will now learn by predicting the next word, not guessing [MASK] tokens."
      ],
      "metadata": {
        "id": "jinukGSrLXup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal language modeling = predict next token\n",
        ")\n"
      ],
      "metadata": {
        "id": "7z_KgVBeLPou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define TrainingArguments"
      ],
      "metadata": {
        "id": "fnaUW9kaLkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-qlora-instruct\",\n",
        "    per_device_train_batch_size=2,            # If you're on Colab T4\n",
        "    gradient_accumulation_steps=4,            # Simulates larger batch\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,                       # Start with 1 for testing\n",
        "    fp16=True,                                # Use GPU precision\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"                          # No WANDB, simple logs\n",
        ")\n"
      ],
      "metadata": {
        "id": "72Fu8CmDLc4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up the Trainer\n",
        "This combines the model, dataset, and training settings into one object that manages training."
      ],
      "metadata": {
        "id": "J1MUONiWLq-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "ySScKLUpLtDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Fine-Tuning"
      ],
      "metadata": {
        "id": "twnAZ1TjL7rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DguRQzjaLxDX",
        "outputId": "63983105-4f3a-4dd0-dde1-e8146dea93b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='402' max='6500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 402/6500 51:20 < 13:02:49, 0.13 it/s, Epoch 0.06/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.473700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.239100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.232800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.052600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.171300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.125600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.154800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.165600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.176400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.182300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.093300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.132200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.167900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.083200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.099400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.185900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.124200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.053600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.076000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.199500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.113200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save LoRA-Tuned Model\n",
        "saves only the LoRA adapter weights"
      ],
      "metadata": {
        "id": "LzdGH_7bQ6Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./mistral-lora-instruct\")\n",
        "tokenizer.save_pretrained(\"./mistral-lora-instruct\")\n"
      ],
      "metadata": {
        "id": "a0M4TWz6L-TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reuse It Later\n",
        "load and use your instruction-tuned model -"
      ],
      "metadata": {
        "id": "_kUM2gyUQ_xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"./mistral-lora-instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-lora-instruct\")\n"
      ],
      "metadata": {
        "id": "UblDSelYQ4Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"### Instruction:\\nTell me a joke.\\n\\n### Response:\\n\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "9rTluZY4RNPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… What we did â€”\n",
        "\n",
        "ðŸ”¹ Step 1: Loaded a Big Pretrained Model (Mistral 7B)\n",
        "You loaded Mistral with:\n",
        "\n",
        "4-bit quantization (very memory-efficient)\n",
        "\n",
        "On free Colab GPU\n",
        "ðŸ”§ This gives a super-smart model but very lightweight!\n",
        "\n",
        "ðŸ”¹ Step 2: Added LoRA Adapters\n",
        "Instead of changing the huge model, added small \"notes\" (LoRA adapters) to:\n",
        "\n",
        "Make tiny changes\n",
        "\n",
        "Only train a few new weights\n",
        "ðŸ“Œ Like writing sticky notes on a textbook instead of rewriting the whole book.\n",
        "\n",
        "ðŸ”¹ Step 3: Loaded the Alpaca Dataset\n",
        "You downloaded instruction examples like:\n",
        "\n",
        "json\n",
        "Copy\n",
        "Edit\n",
        "{\n",
        "  \"instruction\": \"Describe a cat.\",\n",
        "  \"input\": \"\",\n",
        "  \"output\": \"A cat is a small furry animal often kept as a pet.\"\n",
        "}\n",
        "ðŸ“Œ These teach the model how to follow commands.\n",
        "\n",
        "ðŸ”¹ Step 4: Formatted the Data for Instruction Tuning\n",
        "You turned each row into a prompt like:\n",
        "\n",
        "text\n",
        "Copy\n",
        "Edit\n",
        "### Instruction:\n",
        "Describe a cat.\n",
        "\n",
        "### Response:\n",
        "A cat is a small furry animal...\n",
        "ðŸ“Œ This teaches the model to reply like a chatbot.\n",
        "\n",
        "\n",
        "ðŸ”¹ Step 5: Tokenized the Text\n",
        "You converted the text into numbers (tokens) the model understands.\n",
        "\n",
        "ðŸ“Œ This is like translating human words into computer language.\n",
        "\n",
        "\n",
        "ðŸ”¹ Step 6: Prepared the Training Settings\n",
        "You told the model:\n",
        "\n",
        "Use small batches\n",
        "\n",
        "Use 1â€“3 training loops (epochs)\n",
        "\n",
        "Print progress\n",
        "\n",
        "Save the results\n",
        "\n",
        "ðŸ“Œ This is like setting the rules for a classroom session.\n",
        "\n",
        "ðŸ”¹ Step 7: Started Fine-Tuning!\n",
        "You used:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "trainer.train()\n",
        "This:\n",
        "\n",
        "Showed your model hundreds of examples\n",
        "\n",
        "Let it learn to follow instructions\n",
        "\n",
        "Stored the new LoRA adapter (tiny weight updates)\n",
        "\n",
        "ðŸŽ‰ Your model learned to follow instructions like ChatGPT, using your data.\n",
        "\n",
        "ðŸ”¹ Step 8: Saved the Fine-Tuned Model\n",
        "You ran:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.save_pretrained(\"mistral-lora-instruct\")\n",
        "\n",
        "ðŸ“Œ Now we can reuse your smart assistant later!"
      ],
      "metadata": {
        "id": "Z3TPx94cQNp5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3du9BWEQxiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}