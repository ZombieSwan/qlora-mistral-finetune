{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1YFz97O7zvjFpal0fFQ3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZombieSwan/qlora-mistral-finetune/blob/main/qlora-mistral-alpaca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zELK9JgMEvwc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft datasets bitsandbytes accelerate --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "F7HZmspKFG3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n"
      ],
      "metadata": {
        "id": "svMMoOJfJlfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "with open(\"alpaca_data.json\", \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_list(data)\n"
      ],
      "metadata": {
        "id": "26K8vI7wKdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction(example):\n",
        "    if example[\"input\"]:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Input:\\n{example['input']}\\n\\n### Response:\\n\"\n",
        "    else:\n",
        "        prompt = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
        "    return {\"text\": prompt + example[\"output\"]}\n",
        "\n",
        "dataset = dataset.map(format_instruction)\n"
      ],
      "metadata": {
        "id": "QrPU9LxFKhTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ],
      "metadata": {
        "id": "68XBucn1KlG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We have:\n",
        "\n",
        "✅ Downloaded and loaded Alpaca dataset\n",
        "\n",
        "✅ Formatted it as instruction → response\n",
        "\n",
        "✅ Tokenized it for your model"
      ],
      "metadata": {
        "id": "8gDORZYFLICk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up the Data Collator\n",
        "The model will now learn by predicting the next word, not guessing [MASK] tokens."
      ],
      "metadata": {
        "id": "jinukGSrLXup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal language modeling = predict next token\n",
        ")\n"
      ],
      "metadata": {
        "id": "7z_KgVBeLPou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define TrainingArguments"
      ],
      "metadata": {
        "id": "fnaUW9kaLkU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-qlora-instruct\",\n",
        "    per_device_train_batch_size=2,            # If you're on Colab T4\n",
        "    gradient_accumulation_steps=4,            # Simulates larger batch\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=1,                       # Start with 1 for testing\n",
        "    fp16=True,                                # Use GPU precision\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"                          # No WANDB, simple logs\n",
        ")\n"
      ],
      "metadata": {
        "id": "72Fu8CmDLc4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up the Trainer\n",
        "This combines the model, dataset, and training settings into one object that manages training."
      ],
      "metadata": {
        "id": "J1MUONiWLq-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "id": "ySScKLUpLtDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Fine-Tuning"
      ],
      "metadata": {
        "id": "twnAZ1TjL7rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DguRQzjaLxDX",
        "outputId": "63983105-4f3a-4dd0-dde1-e8146dea93b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='402' max='6500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 402/6500 51:20 < 13:02:49, 0.13 it/s, Epoch 0.06/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.473700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.239100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.232800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.052600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.183900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.175200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.146100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.171300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.125600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.154800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.165600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.105000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.176400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.155200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.182300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.115800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.093300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.132200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.167900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.083200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.099400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.185900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.124200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.089400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.053600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.149900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.076000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.199500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.129600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.113200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save LoRA-Tuned Model\n",
        "saves only the LoRA adapter weights"
      ],
      "metadata": {
        "id": "LzdGH_7bQ6Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./mistral-lora-instruct\")\n",
        "tokenizer.save_pretrained(\"./mistral-lora-instruct\")\n"
      ],
      "metadata": {
        "id": "a0M4TWz6L-TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reuse It Later\n",
        "load and use your instruction-tuned model -"
      ],
      "metadata": {
        "id": "_kUM2gyUQ_xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-v0.1\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"./mistral-lora-instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./mistral-lora-instruct\")\n"
      ],
      "metadata": {
        "id": "UblDSelYQ4Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"### Instruction:\\nTell me a joke.\\n\\n### Response:\\n\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "9rTluZY4RNPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ What we did —\n",
        "\n",
        "🔹 Step 1: Loaded a Big Pretrained Model (Mistral 7B)\n",
        "You loaded Mistral with:\n",
        "\n",
        "4-bit quantization (very memory-efficient)\n",
        "\n",
        "On free Colab GPU\n",
        "🔧 This gives a super-smart model but very lightweight!\n",
        "\n",
        "🔹 Step 2: Added LoRA Adapters\n",
        "Instead of changing the huge model, added small \"notes\" (LoRA adapters) to:\n",
        "\n",
        "Make tiny changes\n",
        "\n",
        "Only train a few new weights\n",
        "📌 Like writing sticky notes on a textbook instead of rewriting the whole book.\n",
        "\n",
        "🔹 Step 3: Loaded the Alpaca Dataset\n",
        "You downloaded instruction examples like:\n",
        "\n",
        "json\n",
        "Copy\n",
        "Edit\n",
        "{\n",
        "  \"instruction\": \"Describe a cat.\",\n",
        "  \"input\": \"\",\n",
        "  \"output\": \"A cat is a small furry animal often kept as a pet.\"\n",
        "}\n",
        "📌 These teach the model how to follow commands.\n",
        "\n",
        "🔹 Step 4: Formatted the Data for Instruction Tuning\n",
        "You turned each row into a prompt like:\n",
        "\n",
        "text\n",
        "Copy\n",
        "Edit\n",
        "### Instruction:\n",
        "Describe a cat.\n",
        "\n",
        "### Response:\n",
        "A cat is a small furry animal...\n",
        "📌 This teaches the model to reply like a chatbot.\n",
        "\n",
        "\n",
        "🔹 Step 5: Tokenized the Text\n",
        "You converted the text into numbers (tokens) the model understands.\n",
        "\n",
        "📌 This is like translating human words into computer language.\n",
        "\n",
        "\n",
        "🔹 Step 6: Prepared the Training Settings\n",
        "You told the model:\n",
        "\n",
        "Use small batches\n",
        "\n",
        "Use 1–3 training loops (epochs)\n",
        "\n",
        "Print progress\n",
        "\n",
        "Save the results\n",
        "\n",
        "📌 This is like setting the rules for a classroom session.\n",
        "\n",
        "🔹 Step 7: Started Fine-Tuning!\n",
        "You used:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "trainer.train()\n",
        "This:\n",
        "\n",
        "Showed your model hundreds of examples\n",
        "\n",
        "Let it learn to follow instructions\n",
        "\n",
        "Stored the new LoRA adapter (tiny weight updates)\n",
        "\n",
        "🎉 Your model learned to follow instructions like ChatGPT, using your data.\n",
        "\n",
        "🔹 Step 8: Saved the Fine-Tuned Model\n",
        "You ran:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.save_pretrained(\"mistral-lora-instruct\")\n",
        "\n",
        "📌 Now we can reuse your smart assistant later!"
      ],
      "metadata": {
        "id": "Z3TPx94cQNp5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3du9BWEQxiu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}